---
title: "Generalized Linear Models"
author: "36-290 -- Statistical Research Methodology"
date: "Week 5 Tuesday -- Fall 2020"
output: 
  slidy_presentation:
    font_adjustment: -1
---

Distributions
===

To understand what generalized linear models are and how they work, we need to start with distributions.

A distribution is a mathematical function $f(x \vert \theta)$ where

- $x$ may take on continuous values or discrete values over the domain of $f(x \vert \theta)$;
- $\theta$ is a set of parameters governing the shape of the distribution (e.g., $\theta = \{\mu,\sigma^2\}$ for a normal or Gaussian distribution);
- the $\vert$ symbol means that the shape of the distribution is *conditional* on the values of $\theta$;
- $f(x \vert \theta) \geq 0$ for all $x$; and
- $\sum_x f(x \vert \theta) = 1$ or $\int_x f(x \vert \theta) = 1$.

In practice, if $x$ is continuously valued we often use $f$ to denote the distribution (and call $f$ a probability density function, or pdf), and if $x$ is discretely valued we often use $p$ (and call it a probability mass function, or pmf).

Distributions
===

An example of a pdf and a pmf (a normal distribution and a binomial distribution):
```{r echo=FALSE,fig.height=3,fig.width=3}
library(ggplot2)
x = seq(-4,4,by=0.01)
y = dnorm(x)
ggplot(data=data.frame(x,y),mapping=aes(x=x,y=y)) + geom_line(color="red") + ylab("f(x)") + ggtitle("Normal")
x = 0:10
y = dbinom(x,size=10,prob=0.3)
ggplot(data=data.frame(x,y),mapping=aes(x=x,y=y)) + geom_point(color="red",size=2) + ylab("p(x)") + ggtitle("Binomial")
```

Distributions and Regression
===

The reason we bring up distributions is because linear regression and its generalized variants make assumptions about how observed data are distributed around the true regression line, conditional on a value of $x$.

For instance, for simple linear regression, we assume that for every value of $x$...

- the distribution governing the possible values of $Y$ is a *normal* distribution (note: we capitalize $Y$ because the values are *random variables*, random samples from the distribution);
- the mean of the normal distribution is $\mu(y \vert x) = \beta_0 + \beta_1 x$;
- the variance of the normal distribution is $\sigma^2$, which is a constant (i.e., does not vary with $x$).

Our goal is to estimate the mean: $E[Y \vert x] = \mu(y \vert x) = \beta_0 + \beta_1 x$. The set of means, for all $x$, is the "regression line."

However, just because these assumptions are made in simple linear regression doesn't mean that all linear regression-related models utilize the same assumptions. They don't. When we step back from these assumptions, we enter the realm of the *generalized linear model*.

Maximum Likelihood
===

In generalized regression, the idea is to 

1. assume a (family of) distribution(s) that govern observed response values $Y$; and
2. estimate the parameters $\theta$ of that distribution.

Estimation is done by maximizing the so-called *likelihood function*:
$$
{\cal L} = \prod_{i=1}^n f(Y_i \vert \theta) %~~{\rm or}~~ {\cal L} = \prod_{i=1}^n p(Y_i \vert \theta)
$$
where $n$ is the number of observed data. (You can also maximize $L = \log{\cal L}$, the so-called "log-likelihood.")

Leaving many details under the rug: the maximum is the point at which the derivative of the likelihood function is zero. (You don't need to check the second derivative: wherever the derivative equals zero, it's a maximum value, not a minimum value.) 

*Determining* the value of $\theta$ that achieves the maximum can be difficult; it may require numerical optimization (wherein the computer, using an algorithm, searches over possible values of $\theta$ to find the optimal one).

For linear regression, ${\cal L}$ can be maximized analytically (i.e., by formula), which makes linear regression fast and popular. Through formulae, we can immediately specify estimates for the $\beta$'s and for $\sigma^2$.

When we generalize linear regression, we are usually not so lucky.

Generalization: Example
===

In typical linear regression, the distribution is normal and the domain of $Y \vert x$ is $(-\infty,\infty)$.

What, however, happens if we know that

1. the domain of observed values of the response is actually $[0,\infty]$? and
2. the observed values are *discrete*, with possible values 0, 1, 2, ...

The normal distribution doesn't hold here. Any idea of what distribution could possibly govern $Y \vert x$? (Remember, we might not know truly how $Y \vert x$ is distributed, but any assumption we make has to fit with the limitations imposed by points 1 and 2 above.)

Generalization: Poisson Regression
===

A distribution that fulfills the conditions imposed on the last slide is the *Poisson* distribution, which has a single parameter $\lambda$, which helpfully is the mean of the distribution (as well as the parameter which governs the distribution's shape).

So, when we apply generalized linear regression in this context, we would identify the family as Poisson.

But there's another step in generalization...

Generalization: Link Function
===

Let's keep things in the realm of one predictor. The linear function is
$$
\beta_0 + \beta_1 x \,.
$$
As noted above, the range of this function is $(-\infty,\infty)$. But in our Poisson regression example, we know that $Y$ cannot be negative, *so we need to tranform the linear function somehow* so that the domain of the transformed function is, in this case, $[0,\infty)$. (Or we could punt and use simple linear regression, but our results may not be meaningful, particularly if we estimate ${\hat Y}$ as being negative!)

There is usually no unique transformation, but rather ones that are conventionally used. For this case:
$$
g(\lambda \vert x) = \log(\lambda \vert x) = \beta_0 + \beta_1 x \,.
$$
$g$ is the *link* function. To tie this all together, given response data whose values are limited to being either 0 or positive integers, with no upper bound, we

1. assume that $Y \vert x$ is Poisson distributed;
2. assume that $\lambda \vert x = e^{\beta_0 + \beta_1 x}$; and
3. use numerical optimization to estimate $\beta_0$ and $\beta_1$ by maximizing the likelihood function.

Note that in the end, we still can perform statistical inference, because we can still determine directly how the predicted response varies with linear coefficients.

Exercises
===

What family might be appropriate for...

1. $Y \vert x$ continuous, but bounded between 0 and 1?
2. $Y \vert x$ continuous, but bounded between 0 and $\infty$?
3. $Y \vert x$ discrete, but can only take on the values 0 and 1?
4. $Y \vert x$ discrete, but can only take on the values 0, 1, 2, ..., $n$?

Case 3 is special, as you will see.



