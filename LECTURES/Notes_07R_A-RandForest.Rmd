---
title: "Random Forest"
author: "36-290 -- Statistical Research Methodology"
date: "Week 7 Thursday -- Fall 2020"
output: 
  slidy_presentation:
    font_adjustment: -1
---

Motivation
===

We have previously learned about decision trees. While trees are interpretable, they do have issues:

- They are highly variable. For instance, if you split a dataset in half and grow trees for each half, they can look very different.
- (Related:) They do not generalize as well as other models, i.e., they tend to have higher test-set MSE values.

To counteract these issues, one can utilize *bootstrap aggregation*, or *bagging*. Let's break this down:

- bootstrap: sample the training data *with replacement*
- aggregation: aggregate many trees, each constructed with a different bootstrap sample of the original training set

Bagging: Algorithm
===

```
Specify number of trees: n

For each tree 1->n:
  - construct a bootstrap sample from the training data
  - grow a deep and unpruned tree (i.e., overfit!)
  
Pass test datum through all n trees:
  - if regression: overall prediction is average of those for each tree
  - if classification: by default, predicted class for each tree is the
                       most represented class in the leaf; overall prediction
                       is majority vote
```

Bagging improves prediction accuracy at the expense of interpretability: one can "read" a single tree, but if you have 500 trees, what can you do? 

$\Rightarrow$ You can use a metric dubbed *variable importance*, which is the average improvement in, e.g., RSS or Gini when splits are made on a particular predictor variable. The larger the average improvement, the more important the predictor variable.

Random Forest: Algorithm
===

The random forest algorithm *is* the bagging algorithm, with a tweak.

For each bootstrapped sampled dataset, we randomly select a subset of the predictor variables, and we build the tree using only those variables. By default, $m = \sqrt{p}$. (If $m = p$, then we recover the bagging algorithm.)

$\Rightarrow$ Selecting a variable subset for each tree allows us to get around the issue that if there is a dominant predictor variable, the first split is (almost always) made on that predictor variable. (Subsetting acts to "decorrelate" the different trees.)

Random Forest: Variable Importance
===

| | Regression | Classification |
| --- | --- | ---|
| %IncMSE | YES (but only if importance = TRUE) | NO |
| IncNodePurity | YES | NO  |
| MeanDecreaseGini | NO | YES |
| MeanDecreaseAccuracy | NO | YES |


If we ignore the `type` argument of the `importance()` function:

- Regression: if you do not specify `importance=TRUE`, only `IncNodePurity` is output. Otherwise `%IncMSE` is output in column 1, and `IncNodePurity` is output in column 2.

- Classification: if you do not specify `importance=TRUE`, only `MeanDecreaseGini` is output. Otherwise $p+2$ columns are output; `MeanDecreaseAccuracy` is output in column $p+1$ and `MeanDecreaseGini` is output in column $p+2$.

Considering the `type` argument just complicates things. We won't cover it here.

%IncMSE: Percentage Increase in MSE (Regression)
===

Algorithm:

1. Grow forest. Using OOB data, compute the MSE. Call this MSE.min.
2. For each predictor variable in turn, randomly permute the data values, then repeat Step 1. Call this MSE.ii.
3. %IncMSE for the ii^th variable is 100*(MSE.ii-MSE.min)/(MSE.min).

- Magnitude does not depend on sample size.

- This is analogous to `MeanDecreaseAccuracy`.

IncNodePurity: Increase in Node Purity
===

- How much does a split reduce the RSS? The output value represents the sum over all splits for that variable, averaged over all trees. That value will be larger or smaller depending on whether the dataset has a larger or smaller sample size.

- This is analogous to `MeanDecreaseGini`.

MeanDecreaseGini
===

- How much does a split reduce the Gini coefficient? The output value represents the (weighted?) sum over all splits for that variable, averaged over all trees. That value will be larger or smaller depending on whether the dataset has a larger or smaller sample size.

- This is analogous to `IncNodePurity`.

MeanDecreaseAccuracy
===

- How many more OOB observations are misclassified if we randomly permute the data in the named data frame column, as opposed to not permuting the data? (This is analogous to `%IncMSE`.)

- The magnitude will depend on sample size.

Variable Importance Plot: Example
===

Here we load the redshift data that we looked at in the Random Forest notes:
```{r echo=FALSE}
df = read.csv("https://raw.githubusercontent.com/pefreeman/PSU_2019/master/PhotoZ.csv")
set.seed(808)
s = sample(nrow(df),0.7*nrow(df))
pred.train = df[s,1:6]
pred.test  = df[-s,1:6]
resp.train = df[s,7]
resp.test  = df[-s,7]
if ( require(randomForest) == FALSE ) {
  install.packages("randomForest",repos="https://cloud.r-project.org")
  library(randomForest)
}
out.rf = randomForest(resp.train~.,data=pred.train,importance=TRUE)
```

Variable Importance Plot: Example
===

Below we provide a custom plotting function for plotting variable importance using `ggplot2`. This is very much an undocumented work in progress.
```{r}
ggVarImpPlot = function(importance,type="MSE",shiftFactor=10) {
  if ( require(ggplot2) == FALSE ) { stop("The ggplot2 package is required to execute this function.")}
  if ( require(grid)    == FALSE ) { stop("The grid package is required to execute this function.")}
  
  w = grep(type,colnames(importance))
  if ( length(w) > 0 ) {
    if ( type == "MSE") xlabel = "MSE: Percentage Increase"
    if ( type == "Purity") xlabel = "Increase in Node Purity"
    if ( type == "Gini") xlabel = "Gini: Mean Decrease"
    if ( type == "Accuracy") xlabel = "Accuracy: Mean Decrease"
    o = order(importance[,w])
    x = importance[o,w]
    var = rownames(importance)[o]
  } else {
    xlabel = colnames(importance)[1]
    o = order(importance[,1])
    x = importance[o,1]
    var = rownames(importance)[o]
  }

  n = length(x)
  y = (1:n)/(n+1)
  df = data.frame(x,y)

  p = ggplot(data=df,mapping=aes(x=x,y=y)) + geom_point(size=2) + xlim(0,max(x)) + ylim(0,1) + xlab(xlabel) + 
    theme(axis.title.y=element_blank(),axis.text.y=element_blank(),axis.ticks.y=element_blank()) +
    theme(plot.margin=unit(c(1,1,1,2),"cm"))
  for ( ii in 1:n ) {
    p = p + annotation_custom(grob = textGrob(label=var[ii]),ymin=y[ii],ymax=y[ii],xmin=-max(x)/shiftFactor,xmax=-max(x)/shiftFactor) +
      geom_hline(yintercept=y[ii],linetype="dashed",color="red",size=0.3)
  }
  gt <- ggplot_gtable(ggplot_build(p))
  gt$layout$clip[gt$layout$name == "panel"] <- "off"
  grid.draw(gt)
  invisible(list(p=p,gt=gt))
}

ggVarImpPlot(importance(out.rf))
```


