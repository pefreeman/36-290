---
title: "Decision Trees"
author: "36-290 -- Statistical Research Methodology"
date: "Week 7 Tuesday -- Fall 2020"
output: 
  slidy_presentation:
    font_adjustment: -1
---

Decision Tree: In Words
===

A decision tree is a model that segments a predictor space into disjoint $p$-dimensional hyper-rectangles, where $p$ is the number of predictor variables.

- For a regression tree, the predicted response in a hyper-rectangle is the average of the response values in that hyper-rectangle.

- For a classification tree, by default the predicted class in a hyper-rectangle is that class that is most represented in that hyper-rectangle.

<center>![](http://www.stat.cmu.edu/~pfreeman/Figure_8.3.png){width=60%}</center>

(Figure 8.3, *Introduction to Statistical Learning* by James et al.)

Decision Tree: Should I Use This Model?
===

Yes:

- It is easy to explain to non-statisticians.
- It is easy to visualize (and thus easy to interpret).

No:

- Trees do not generalize as well as other models (i.e., they tend to have higher test-set MSEs).

Decision Tree: Detail
===

<center>![](http://www.stat.cmu.edu/~pfreeman/Algorithm_8.1.png){width=50%}</center>

(Algorithm 8.1, *Introduction to Statistical Learning* by James et al.)

While the algorithm given above is for a regression tree, the classification tree algorithm is similar: instead of splits based on reduction of the residual sum-of-squares (RSS), the splits would be based on, e.g., reduction of the Gini coefficient, which is a metric that becomes smaller as each node becomes more "pure," i.e., populated more and more by objects of a single class.

In a perfect world, one would systematically try all possible combinations of hyper-rectangles to see which combination minimizes values of RSS/Gini. However, our world is imperfect; the decision tree algorithm is a greedy algorithm which utilizes top-down *recursive binary splitting* to build the model: while each split is a "locally optimal" one to make (i.e., it causes the largest reduction in RSS or Gini), the final model may not be "globally optimal" (i.e., it may not have the smallest possible overall RSS or Gini value).

To enlarge upon Step 1 above, splitting may cease not only when the number of data in a terminal node/hyper-rectangle is smaller than some threshold value, but also when the reduction in the RSS or Gini caused by splitting is smaller than some specified minimum value.

Decision Tree: Detail
===

<center>![](http://www.stat.cmu.edu/~pfreeman/Algorithm_8.1.png){width=50%}</center>

When building a decision tree, one must guard against overfitting. For instance, a tree that places a hyper-rectangle around each datum will be highly flexible (with training set MSE or MCR equal to zero!) but will not generalize well. One strategy for dealing with overfitting is to grow a large tree, then apply *cost complexity* (or *weakest link*) pruning (as described in Steps 2-4 above).

