---
title: "Variable Selection"
author: "36-290 -- Statistical Research Methodology"
date: "Week 6 Tuesday -- Fall 2021"
output:
  xaringan::moon_reader:
    yolo: false
    nature:
      beforeInit: "macros.js"
---

<style type="text/css">
.remark-slide-content {
    font-size: 16px;
    padding: 1em 4em 1em 4em;
}
.remark-code {
    font-size: 12px;
}
</style>

```{css,echo=FALSE}
table {
  font-size: 12px;
}
```

## The Setting

We wish to learn a linear model. Our estimate is
$$\hat{Y} \vert \mathbf{x} = \hat{\beta}_0 + \sum_{i=1}^p \hat{\beta}_i x_i \,,$$
where the hats denote estimated quantities.

In subset selection, we attempt to select a subset $s$ out of the $p$ overall predictors. Why?

1. *To improve prediction accuracy*. Eliminating uninformative predictors can lead to lower model variance, at the expense of a slight increase in bias, leading to lower MSE values.

2. *To improve model interpretability*. Eliminating uninformative predictors is obviously a good thing when your goal is to tell the story of how your predictors are associated with your response.

Note that subset selection is useful and/or necessary if, e.g., $n \lesssim p$ (the sample size is roughly the same as, or less than, the number of predictor variables), but can still be helpful if $n > p$. As $n/p \rightarrow \infty$, subset selection will yield less and less "useful" results, i.e., it will eliminate fewer and fewer variables. However, you should still try it even when $n \gg p$: you won't know for sure how useful subset selection is otherwise!

---

## Best Subset Selection

.center[
  ![:scale 70%](http://www.stat.cmu.edu/~pfreeman/Algorithm_6.1.png)
]
(Algorithm 6.1, *Introduction to Statistical Learning* by James et al.)

Note:
$${p \choose k} = \frac{p!}{k!(p-k)!} \,.$$
For multiple linear regression, BSS works for $p \lesssim 25$; otherwise the total number of models is such that lack of computer memory becomes an issue. For logistic regression, BSS is limited to $p \leq 15$ due to the computational costs of having to perform numerical optimization. (And BSS is *slow* near that upper limit.)

---

## Best Subset Selection: Tuning

The application of BSS involves tuning: what is the best set of variables to keep? When tuning is involved, we generally have to split the training data into a smaller training set plus a so-called *validation* set, or perform cross-validation on the training set. 

(For instance, if we originally used 70% of our data to train the model and 30% to test it, after resplitting the training data we might have 49% [70% of 70%] of our data used for training, 21% [30% of 70%] for validation, and 30% for testing.)

However, here we don't need to explicitly resplit the data: the first three metrics listed under Step 3 above ( $C_p$, AIC, and BIC) are all estimators of the validation set MSE. So we can apply BSS to our full training dataset!

---

## Best Subset Selection: Metrics

The functional forms of the metrics given in Step 3 are
\begin{eqnarray*}
C_p &=& \frac{1}{n} ( {\rm RSS} + 2k\hat{\sigma}^2 ) \\
{\rm AIC} &=& \frac{1}{n \hat{\sigma}^2} ( {\rm RSS} + 2k \hat{\sigma}^2 ) = \frac{C_p}{\hat{\sigma}^2} \\
{\rm BIC} &=& \frac{1}{n} ( {\rm RSS} + \log(n)k\hat{\sigma}^2 )
\end{eqnarray*}
RSS denotes the "residual sum-of-squares." The additive terms are penalty terms that increase with $k$ and thus act to prevent overfitting. $\hat{\sigma}^2$ is an estimate of the variance of the linear regression error term $\epsilon$, i.e., the variance of the scatter of data around the regression line (thus the metrics do implicitly assume constant error).

---

## Best Subset Selection: Metrics

Typically, $\log(n) > 2$, so BIC (or "Bayesian Information Criterion") imposes a larger penalty relative to $C_p$ (or "Mallow's $C_p$") or AIC (or "Akaike Information Criterion").

$\Rightarrow$ BIC tends to underfit the data (i.e., it will select as optimal those models that have *fewer* variables)

$\Rightarrow$ $C_p$ and AIC tend to overfit the data (i.e., they will select models with *more* variables)

Which metric you choose is up to you; the choice should be motivated by your inferential goals. (This is another one of those "Embrace the Ambiguity" moments.) 

- If you use BIC, then you can be confident that every selected variable is important, but other important variables might have been left out of the final list. 

- If you use, e.g., AIC, then you can be confident that your selected variables include all the important ones, but the final list may also include some unimportant ones as well.

(What about adjusted $R^2$? The link between that metric and the validation-set MSE is not theoretically well motivated, so one should only use BIC or $C_p$/AIC to select the variable subset.)

---

## Forward and Backward Stepwise Selection

What if BSS is computationally infeasible? In that case, we might use either *forward* or *backward stepwise selection*. For instance:

.center[
  ![:scale 70%](http://www.stat.cmu.edu/~pfreeman/Algorithm_6.2.png)
]
(Algorithm 6.2, *Introduction to Statistical Learning* by James et al.)

In words, forward stepwise selection starts with no predictor variables and adds one at a time; backward stepwise selection is similar, except that it starts with the full set of predictors and takes one out at a time. One can apply forward and backward stepwise selection using `regsubsets()` or `bestglm()` as above, but with the arguments `method="forward"` or `method="backward"`.

Forward and backward stepwise selection are examples of *greedy algorithms*: they make locally optimally choices that may collectively not yield a globally optimal solution. BSS is always to be preferred, if applying it is computationally feasible.

---

## Regression Example

```{r echo=FALSE}
load(url("https://raw.githubusercontent.com/pefreeman/36-290/master/EXAMPLE_DATASETS/PHOTO_MORPH/photo_morph.Rdata"))
df = data.frame(predictors,"y"=response)
```
`df` is a data frame with 3,419 rows and 17 columns. The response variable is contained in a column dubbed `y`, which is what the `bestglm` package expects.
```{r}
# Perform 70-30 data splitting.
set.seed(404)
train = sample(nrow(df),0.7*nrow(df))
df.train = df[train,]
df.test  = df[-train,]
suppressMessages(library(bestglm))
bg.out = bestglm(df.train,family=gaussian,IC="BIC")
bg.out$BestModel
```
We observe that 9 of 16 predictor variables are retained when using BIC as the penalizing criterion. (If we were to use AIC instead? 11 variables are retained.)

---

## Regression Example

```{r fig.height=4,fig.width=4,fig.align="center"}
library(ggplot2)
df.bg = data.frame(1:16,bg.out$Subsets$BIC[-1])
names(df.bg) = c("p","BIC")
ggplot(data=df.bg,mapping=aes(x=p,y=BIC)) + 
  geom_point() + geom_line() + ylim(-2900,-2700)
```

---

## Regression Example

The output of `bestglm()` contains, as you saw above, `BestModel`. According to the documentation for `bestglm()`, `BestModel` is "[a]n lm-object representing the best fitted algorithm." That means you can pass it to `predict()` in order to generate predicted response values (where the response is in the `y` column of your data frames).
```{r}
resp.pred = predict(bg.out$BestModel,newdata=df.test)
mean((df.test$y-resp.pred)^2)  # compare with 0.2658 for full predictor set
```

---

## Classification Example

The dataset that we read in below contains magnitude and redshift data for 500 quasars and 500 stars. The idea is to learn a classifier that can discriminate between quasars and stars with a low misclassification rate.
```{r}
df = read.csv("https://raw.githubusercontent.com/pefreeman/PSU_2019/master/StarQuasar.csv")
names(df)[8] = "y"                                         # necessary tweak for bestglm: response is "y"
df$y = factor(df$y)
set.seed(202)
s = sample(nrow(df),0.7*nrow(df))
df.train = df[s,c(1:5,8)]                                  # don't include redshift or redshift error!
df.test  = df[-s,c(1:5,8)]
```

---

## Classification Example

```{r}
library(bestglm)
bg.out = bestglm(df.train,family=binomial,IC="BIC")
bg.out$BestModel
```
We retain four of the five predictor variables (only dropping `r.mag`).

```{r}
resp.prob = predict(bg.out$BestModel,newdata=df.test,type="response")
resp.pred = ifelse(resp.prob>0.5,"STAR","QSO")
mean(resp.pred!=df.test$y) ; table(resp.pred,df.test$y)
```

---

## Classification Example

`bestglm` will not perform forward- or backward-stepwise selection in a logistic regression setting if $p > 15$. Here is a code that will do forward-stepwise selection. Note that it assumes the use of AIC; changing to BIC is not a simple change.

```
log_forward = function(pred.train,resp.train)
{
  var.num = ncol(pred.train)
  var.keep = aic.keep = c()
  var.rem = 1:var.num

  var = 0
  while ( var < var.num ) {
    var = var+1
    aic.tmp = rep(0,length(var.rem))
    for ( ii in 1:length(var.rem) ) {
      var.set = c(var.keep,var.rem[ii])
      df = pred.train[,var.set]
      if ( var == 1 ) df = data.frame(df)
      aic.tmp[ii] = summary(suppressWarnings(glm(resp.train~.,data=df,family=binomial)))$aic
    }
    if ( length(aic.keep) == 0 || min(aic.tmp) < min(aic.keep) ) {
      aic.keep = append(aic.keep,min(aic.tmp))
      w = which.min(aic.tmp)
      var.keep = append(var.keep,var.rem[w])
      var.rem = var.rem[-w]
    } else {
      break
    }
  }
  return(sort(names(pred.train[var.keep])))
}
```

---

## Classification Example

For completeness, here is the backward-stepwise analogue:

```
log_backward = function(pred.train,resp.train)
{
  var.num = ncol(pred.train)
  var.keep = 1:var.num
  var.rem = aic.rem = c()

  var = var.num
  while ( var > 1 ) {
    aic.tmp = rep(0,length(var.keep)-1)
    for ( ii in 1:(length(var.keep)-1) ) {
      var.set = var.keep[-ii]
      df = pred.train[,var.set]
      if ( var == 2 ) df = data.frame(df)
      aic.tmp[ii] = summary(suppressWarnings(glm(resp.train~.,data=df,family=binomial)))$aic
    }
    if ( length(aic.rem) == 0 || min(aic.tmp) < min(aic.rem) ) {
      aic.rem = append(aic.rem,min(aic.tmp))
      w = which.min(aic.tmp)
      var.rem = append(var.rem,var.keep[w])
      var.keep = var.keep[-w]
      var = var-1
    } else {
      break
    }
  }
  return(sort(names(pred.train[-var.rem])))
}
```



